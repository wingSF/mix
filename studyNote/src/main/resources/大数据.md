# 新手入门
* [hadoop](http://hadoop.apache.org/)
    * windows搭建的过程中,hadoop-env.cmd中对JAVA_HOME的引用，一般存在于Programma Files目录下，由于该空格的存在会导致异常,采用如下方式修改
    > set JAVA_HOME=C:\PROGRA~1\Java\jdk1.7.0_67  
* [flume](http://flume.apache.org/)
    //todo
    * sink
* [parquet](http://parquet.apache.org/)
    * 列式存储的数据结构,适合以列为维度分析数据
        * 先按照某列进行查询，在大量数据的时候，io的内容更少，效率更高
        * parquet的数据格式，可以对数据进行合理的压缩，在大量数据的时候，相比于普通文件占用空间极小
* 上手第一个问题
    * 日志上传由设备的联网决定，使用类s3的服务对日志文件进行存储，以apptoken区分app类型，eventname区分事件类型，dt作为日期。由于上传时间的不确定性，会导致在s3上面出现很多特别小的日志文件。在执行map，reduce任务的时候，会出现长时间的等待。(hadoop best practice 每个文件128M)
    > 如何合理的解决该问题，写入端已经使用了缓冲刷新机制，但是由于日志回落(当日上传前几日的日志)的原因，还是会出现小文件。暂时采用java写一套任务调度系统，用于将小文件进行合并。